<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="utf-8">
  

  
  <title>Lucene索引文件格式--详细篇 | Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="#Lucene索引文件格式–详细篇 Lucene 的索引里面存了些什么,如何存放的,也即 Lucene 的索引文件格式,是读懂 Lucene 源代码的一把钥匙。当我们真正进入到 Lucene 源代码之中的时候,我们会发现:  Lucene 的索引过程,就是按照全文检索的基本过程,将倒排表写成此文件格式的过程。  Lucene 的搜索过程,就是按照此文件格式将索引进去的信息读出来,然后计算每篇文档打">
<meta name="keywords" content="Lucene">
<meta property="og:type" content="article">
<meta property="og:title" content="Lucene索引文件格式--详细篇">
<meta property="og:url" content="http://yoursite.com/2016/04/09/Lucene/Lucene索引文件格式--详细篇/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="#Lucene索引文件格式–详细篇 Lucene 的索引里面存了些什么,如何存放的,也即 Lucene 的索引文件格式,是读懂 Lucene 源代码的一把钥匙。当我们真正进入到 Lucene 源代码之中的时候,我们会发现:  Lucene 的索引过程,就是按照全文检索的基本过程,将倒排表写成此文件格式的过程。  Lucene 的搜索过程,就是按照此文件格式将索引进去的信息读出来,然后计算每篇文档打">
<meta property="og:locale" content="default">
<meta property="og:image" content="http://oawztil0a.bkt.clouddn.com/Blog/Lucene/Lucene%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6%E6%A0%BC%E5%BC%8F1.png">
<meta property="og:image" content="http://oawztil0a.bkt.clouddn.com/Blog/Lucene/Lucene%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6%E6%A0%BC%E5%BC%8F2.png">
<meta property="og:image" content="http://oawztil0a.bkt.clouddn.com/Blog/Lucene/Lucene%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6%E6%A0%BC%E5%BC%8F3.png">
<meta property="og:image" content="http://oawztil0a.bkt.clouddn.com/Blog/Lucene/Lucene%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6%E6%A0%BC%E5%BC%8F4.png">
<meta property="og:image" content="http://oawztil0a.bkt.clouddn.com/Blog/Lucene/Lucene%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6%E6%A0%BC%E5%BC%8F5.png">
<meta property="og:image" content="http://oawztil0a.bkt.clouddn.com/Blog/Lucene/Lucene%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6%E6%A0%BC%E5%BC%8F6.png">
<meta property="og:image" content="http://oawztil0a.bkt.clouddn.com/Blog/Lucene/Lucene%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6%E6%A0%BC%E5%BC%8F7.png">
<meta property="og:image" content="http://oawztil0a.bkt.clouddn.com/Blog/Lucene/Lucene%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6%E6%A0%BC%E5%BC%8F8.png">
<meta property="og:image" content="http://oawztil0a.bkt.clouddn.com/Blog/Lucene/Lucene%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6%E6%A0%BC%E5%BC%8F9.png">
<meta property="og:image" content="http://oawztil0a.bkt.clouddn.com/Blog/Lucene/Lucene%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6%E6%A0%BC%E5%BC%8F10.png">
<meta property="og:image" content="http://oawztil0a.bkt.clouddn.com/Blog/Lucene/Lucene%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6%E6%A0%BC%E5%BC%8F11.png">
<meta property="og:image" content="http://oawztil0a.bkt.clouddn.com/Blog/Lucene/Lucene%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6%E6%A0%BC%E5%BC%8F12.png">
<meta property="og:image" content="http://oawztil0a.bkt.clouddn.com/Blog/Lucene/Lucene%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6%E6%A0%BC%E5%BC%8F13.png">
<meta property="og:image" content="http://oawztil0a.bkt.clouddn.com/Blog/Lucene/Lucene%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6%E6%A0%BC%E5%BC%8F14.png">
<meta property="og:image" content="http://oawztil0a.bkt.clouddn.com/Blog/Lucene/Lucene%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6%E6%A0%BC%E5%BC%8F15.png">
<meta property="og:image" content="http://oawztil0a.bkt.clouddn.com/Blog/Lucene/Lucene%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6%E6%A0%BC%E5%BC%8F16.png">
<meta property="og:image" content="http://oawztil0a.bkt.clouddn.com/Blog/Lucene/Lucene%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6%E6%A0%BC%E5%BC%8F17.png">
<meta property="og:image" content="http://oawztil0a.bkt.clouddn.com/Blog/Lucene/Lucene%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6%E6%A0%BC%E5%BC%8F18.png">
<meta property="og:image" content="http://oawztil0a.bkt.clouddn.com/Blog/Lucene/Lucene%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6%E6%A0%BC%E5%BC%8F19.png">
<meta property="og:image" content="http://oawztil0a.bkt.clouddn.com/Blog/Lucene/Lucene%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6%E6%A0%BC%E5%BC%8F20.png">
<meta property="og:image" content="http://oawztil0a.bkt.clouddn.com/Blog/Lucene/Lucene%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6%E6%A0%BC%E5%BC%8F21.png">
<meta property="og:image" content="http://oawztil0a.bkt.clouddn.com/Blog/Lucene/Lucene%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6%E6%A0%BC%E5%BC%8F22.png">
<meta property="og:image" content="http://oawztil0a.bkt.clouddn.com/Blog/Lucene/Lucene%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6%E6%A0%BC%E5%BC%8F23.png">
<meta property="og:image" content="http://oawztil0a.bkt.clouddn.com/Blog/Lucene/Lucene%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6%E6%A0%BC%E5%BC%8F24.png">
<meta property="og:image" content="http://oawztil0a.bkt.clouddn.com/Blog/Lucene/Lucene%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6%E6%A0%BC%E5%BC%8F25.png">
<meta property="og:image" content="http://oawztil0a.bkt.clouddn.com/Blog/Lucene/Lucene%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6%E6%A0%BC%E5%BC%8F26.png">
<meta property="og:image" content="http://oawztil0a.bkt.clouddn.com/Blog/Lucene/Lucene%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6%E6%A0%BC%E5%BC%8F27.png">
<meta property="og:image" content="http://oawztil0a.bkt.clouddn.com/Blog/Lucene/Lucene%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6%E6%A0%BC%E5%BC%8F28.png">
<meta property="og:image" content="http://oawztil0a.bkt.clouddn.com/Blog/Lucene/Lucene%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6%E6%A0%BC%E5%BC%8F29.png">
<meta property="og:updated_time" content="2017-05-24T08:37:29.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Lucene索引文件格式--详细篇">
<meta name="twitter:description" content="#Lucene索引文件格式–详细篇 Lucene 的索引里面存了些什么,如何存放的,也即 Lucene 的索引文件格式,是读懂 Lucene 源代码的一把钥匙。当我们真正进入到 Lucene 源代码之中的时候,我们会发现:  Lucene 的索引过程,就是按照全文检索的基本过程,将倒排表写成此文件格式的过程。  Lucene 的搜索过程,就是按照此文件格式将索引进去的信息读出来,然后计算每篇文档打">
<meta name="twitter:image" content="http://oawztil0a.bkt.clouddn.com/Blog/Lucene/Lucene%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6%E6%A0%BC%E5%BC%8F1.png">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
</head>
</html>
<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-Lucene/Lucene索引文件格式--详细篇" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2016/04/09/Lucene/Lucene索引文件格式--详细篇/" class="article-date">
  <time datetime="2016-04-09T12:38:36.000Z" itemprop="datePublished">2016-04-09</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Lucene/">Lucene</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      Lucene索引文件格式--详细篇
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>#Lucene索引文件格式–详细篇</p>
<p>Lucene 的索引里面存了些什么,如何存放的,也即 Lucene 的索引文件格式,是读懂 Lucene 源代码的一把钥匙。<br>当我们真正进入到 Lucene 源代码之中的时候,我们会发现:</p>
<ul>
<li>Lucene 的索引过程,就是按照全文检索的基本过程,将倒排表写成此文件格式的过程。 </li>
<li>Lucene 的搜索过程,就是按照此文件格式将索引进去的信息读出来,然后计算每篇文<br>档打分(score)的过程。</li>
</ul>
<p>本文详细解读了 Apache Lucene - Index File Formats(<a href="http://lucene.apache.org/java/2_9_0/fileformats.html" target="_blank" rel="noopener">http://lucene.apache.org/java/2_9_0/fileformats.html</a>) 这篇文章。<br><a id="more"></a></p>
<h2 id="一、基本概念"><a href="#一、基本概念" class="headerlink" title="一、基本概念"></a>一、基本概念</h2><p>下图就是 Lucene 生成的索引的一个实例:<br><img src="http://oawztil0a.bkt.clouddn.com/Blog/Lucene/Lucene%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6%E6%A0%BC%E5%BC%8F1.png" alt></p>
<h3 id="1、Lucene的索引结构的层次"><a href="#1、Lucene的索引结构的层次" class="headerlink" title="1、Lucene的索引结构的层次"></a>1、Lucene的索引结构的层次</h3><ul>
<li>索引(Index)：<ul>
<li>在Lucene中一个索引是放在一个文件夹中的。</li>
<li>如上图，同一文件夹中的所有的文件构成一个Lucene索引。</li>
</ul>
</li>
<li>段(Segment)：<ul>
<li>一个索引可以包含多个段，段与段之间是独立的，添加新文档可以生成新的段，不同的段可以合并。</li>
<li>如上图，具有相同前缀文件的属同一个段，图中共两个段 “_0” 和 “_1”。</li>
<li>segments.gen和segments_5是段的元数据文件，也即它们保存了段的属性信息。</li>
</ul>
</li>
<li>文档(Document)：<ul>
<li>文档是我们建索引的基本单位，不同的文档是保存在不同的段中的，一个段可以包含多篇文档。</li>
<li>新添加的文档是单独保存在一个新生成的段中，随着段的合并，不同的文档合并到同一个段中。</li>
</ul>
</li>
<li>域(Field)：<ul>
<li>一篇文档包含不同类型的信息，可以分开索引，比如标题，时间，正文，作者等，都可以保存在不同的域里。</li>
<li>不同域的索引方式可以不同，在真正解析域的存储的时候，我们会详细解读。</li>
</ul>
</li>
<li>词(Term)：<ul>
<li>词是索引的最小单位，是经过词法分析和语言处理后的字符串。</li>
</ul>
</li>
</ul>
<h3 id="2、Lucene的索引结构中信息保存方式"><a href="#2、Lucene的索引结构中信息保存方式" class="headerlink" title="2、Lucene的索引结构中信息保存方式"></a>2、Lucene的索引结构中信息保存方式</h3><p>Lucene的索引结构中，即保存了正向信息，也保存了反向信息。</p>
<p><strong>正向信息：</strong></p>
<ul>
<li>按层次保存了从索引，一直到词的包含关系：索引(Index) –&gt; 段(segment) –&gt; 文档(Document) –&gt; 域(Field) –&gt; 词(Term)</li>
<li>也即此索引包含了那些段，每个段包含了那些文档，每个文档包含了那些域，每个域包含了那些词。</li>
<li>既然是层次结构，则每个层次都保存了本层次的信息以及下一层次的元信息，也即属性信息，比如一本介绍中国地理的书，应该首先介绍中国地理的概况，以及中国包含多少个省，每个省介绍本省的基本概况及包含多少个市，每个市介绍本市的基本概况及包含多少个县，每个县具体介绍每个县的具体情况。</li>
<li>如上图，包含正向信息的文件有：<ul>
<li>segments_N保存了此索引包含多少个段，每个段包含多少篇文档。</li>
<li>XXX.fnm保存了此段包含了多少个域，每个域的名称及索引方式。</li>
<li>XXX.fdx，XXX.fdt保存了此段包含的所有文档，每篇文档包含了多少域，每个域保存了那些信息。</li>
<li>XXX.tvx，XXX.tvd，XXX.tvf保存了此段包含多少文档，每篇文档包含了多少域，每个域包含了多少词，每个词的字符串，位置等信息。</li>
</ul>
</li>
</ul>
<p><strong>反向信息：</strong></p>
<ul>
<li>保存了词典到倒排表的映射：词(Term) –&gt; 文档(Document)</li>
<li>如上图，包含反向信息的文件有：<ul>
<li>XXX.tis，XXX.tii保存了词典(Term Dictionary)，也即此段包含的所有的词按字典顺序的排序。</li>
<li>XXX.frq保存了倒排表，也即包含每个词的文档ID列表。</li>
<li>XXX.prx保存了倒排表中每个词在包含此词的文档中的位置。</li>
</ul>
</li>
</ul>
<p>在了解Lucene索引的详细结构之前，先看看Lucene索引中的基本数据类型。</p>
<h2 id="二、基本类型"><a href="#二、基本类型" class="headerlink" title="二、基本类型"></a>二、基本类型</h2><p>Lucene索引文件中，用一下基本类型来保存信息：</p>
<ul>
<li>Byte：是最基本的类型，长8位(bit)。</li>
<li>UInt32：由4个Byte组成。</li>
<li>UInt64：由8个Byte组成。</li>
<li>Chars：是UTF-8编码的一系列Byte。</li>
<li>String：一个字符串首先是一个VInt来表示此字符串包含的字符的个数，接着便是UTF-8编码的字符序列Chars。</li>
<li>VInt：<ul>
<li>变长的整数类型，它可能包含多个Byte，对于每个Byte的8位，其中后7位表示数值，最高1位表示是否还有另一个Byte，0表示没有，1表示有。</li>
<li>越前面的Byte表示数值的低位，越后面的Byte表示数值的高位。</li>
<li>例如130化为二进制为 1000, 0010，总共需要8位，一个Byte表示不了，因而需要两个Byte来表示，第一个Byte表示后7位，并且在最高位置1来表示后面还有一个Byte，所以为(1) 0000010，第二个Byte表示第8位，并且最高位置0来表示后面没有其他的Byte了，所以为(0) 0000001。<img src="http://oawztil0a.bkt.clouddn.com/Blog/Lucene/Lucene%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6%E6%A0%BC%E5%BC%8F2.png" alt></li>
</ul>
</li>
</ul>
<h2 id="三、基本规则"><a href="#三、基本规则" class="headerlink" title="三、基本规则"></a>三、基本规则</h2><p>Lucene为了使的信息的存储占用的空间更小，访问速度更快，采取了一些特殊的技巧，然而在看Lucene文件格式的时候，这些技巧却容易使我们感到困惑，所以把这些特殊的技巧规则提取出来介绍一下。</p>
<h3 id="1、前缀后缀规则-Prefix-Suffix"><a href="#1、前缀后缀规则-Prefix-Suffix" class="headerlink" title="1、前缀后缀规则(Prefix+Suffix)"></a>1、前缀后缀规则(Prefix+Suffix)</h3><p>Lucene在反向索引中，要保存词典(Term Dictionary)的信息，所有的词(Term)在词典中是按照字典顺序进行排列的，然而词典中包含了文档中的几乎所有的词，并且有的词还是非常的长的，这样索引文件会非常的大，所谓前缀后缀规则，即当某个词和前一个词有共同的前缀的时候，后面的词仅仅保存前缀在词中的偏移(offset)，以及除前缀以外的字符串(称为后缀)。<br><img src="http://oawztil0a.bkt.clouddn.com/Blog/Lucene/Lucene%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6%E6%A0%BC%E5%BC%8F3.png" alt></p>
<p>比如要存储如下词：term，termagancy，termagant，terminal，</p>
<p>如果按照正常方式来存储，需要的空间如下：<br>[VInt = 4] [t][e][r][m]<br>[VInt = 10][t][e][r][m][a][g][a][n][c][y]<br>[VInt = 9][t][e][r][m][a][g][a][n][t]<br>[VInt = 8][t][e][r][m][i][n][a][l]<br>共需要35个Byte.</p>
<p>如果应用前缀后缀规则，需要的空间如下：<br>[VInt = 4] [t][e][r][m]<br>[VInt = 4 (offset)][VInt = 6][a][g][a][n][c][y]<br>[VInt = 8 (offset)][VInt = 1][t]<br>[VInt = 4(offset)][VInt = 4][i][n][a][l]<br>共需要22个Byte。</p>
<p>大大缩小了存储空间，尤其是在按字典顺序排序的情况下，前缀的重合率大大提高。</p>
<h3 id="2、差值规则-Delta"><a href="#2、差值规则-Delta" class="headerlink" title="2、差值规则(Delta)"></a>2、差值规则(Delta)</h3><p>在Lucene的反向索引中，需要保存很多整型数字的信息，比如文档ID号，比如词(Term)在文档中的位置等等。</p>
<p>由上面介绍，我们知道，整型数字是以VInt的格式存储的。随着数值的增大，每个数字占用的Byte的个数也逐渐的增多。所谓差值规则(Delta)就是先后保存两个整数的时候，后面的整数仅仅保存和前面整数的差即可。<br><img src="http://oawztil0a.bkt.clouddn.com/Blog/Lucene/Lucene%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6%E6%A0%BC%E5%BC%8F4.png" alt></p>
<p>比如要存储如下整数：16386，16387，16388，16389</p>
<p>如果按照正常方式来存储，需要的空间如下：<br>16386：[(1) 000, 0010][(1) 000, 0000][(0) 000, 0001]<br>16387：[(1) 000, 0011][(1) 000, 0000][(0) 000, 0001]<br>16388：[(1) 000, 0100][(1) 000, 0000][(0) 000, 0001]<br>16389：[(1) 000, 0101][(1) 000, 0000][(0) 000, 0001]<br>供需12个Byte。</p>
<p>如果应用差值规则来存储，需要的空间如下：</p>
<p>[(1) 000, 0010][(1) 000, 0000][(0) 000, 0001]<br>[(0) 000, 0001]<br>[(0) 000, 0001]<br>[(0) 000, 0001]<br>共需6个Byte。</p>
<p>大大缩小了存储空间，而且无论是文档ID，还是词在文档中的位置，都是按从小到大的顺序，逐渐增大的。</p>
<h3 id="3、或然跟随规则-A-B"><a href="#3、或然跟随规则-A-B" class="headerlink" title="3、或然跟随规则(A, B?)"></a>3、或然跟随规则(A, B?)</h3><p>Lucene的索引结构中存在这样的情况，某个值A后面可能存在某个值B，也可能不存在，需要一个标志来表示后面是否跟随着B。</p>
<p>一般的情况下，在A后面放置一个Byte，为0则后面不存在B，为1则后面存在B，或者0则后面存在B，1则后面不存在B。</p>
<p>但这样要浪费一个Byte的空间，其实一个Bit就可以了。</p>
<p>在Lucene中，采取以下的方式：A的值左移一位，空出最后一位，作为标志位，来表示后面是否跟随B，所以在这种情况下，A/2是真正的A原来的值。<br><img src="http://oawztil0a.bkt.clouddn.com/Blog/Lucene/Lucene%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6%E6%A0%BC%E5%BC%8F5.png" alt></p>
<p>如果去读Apache Lucene - Index File Formats这篇文章，会发现很多符合这种规则的：</p>
<ul>
<li>.frq文件中的DocDelta[, Freq?]，DocSkip,PayloadLength?</li>
<li>.prx文件中的PositionDelta,Payload? (但不完全是，如下表分析)</li>
</ul>
<p>当然还有一些带?的但不属于此规则的：</p>
<ul>
<li>.frq文件中的SkipChildLevelPointer?，是多层跳跃表中，指向下一层表的指针，当然如果是最后一层，此值就不存在，也不需要标志。</li>
<li>.tvf文件中的Positions?, Offsets?。d<ul>
<li>在此类情况下，带?的值是否存在，并不取决于前面的值的最后一位。</li>
<li>而是取决于Lucene的某项配置，当然这些配置也是保存在Lucene索引文件中的。</li>
<li>如Position和Offset是否存储，取决于.fnm文件中对于每个域的配置(TermVector.WITH_POSITIONS和TermVector.WITH_OFFSETS)</li>
</ul>
</li>
</ul>
<p>为什么会存在以上两种情况，其实是可以理解的：</p>
<ul>
<li>对于符合或然跟随规则的，是因为对于每一个A，B是否存在都不相同，当这种情况大量存在的时候，从一个Byte到一个Bit如此8倍的空间节约还是很值得的。</li>
<li>对于不符合或然跟随规则的，是因为某个值的是否存在的配置对于整个域(Field)甚至整个索引都是有效的，而非每次的情况都不相同，因而可以统一存放一个标志。</li>
</ul>
<blockquote>
<p>文章中对如下格式的描述令人困惑：<br>Positions –&gt; &lt;PositionDelta,Payload?&gt; Freq<br>Payload –&gt; &lt;PayloadLength?,PayloadData&gt;<br>PositionDelta和Payload是否适用或然跟随规则呢？如何标识PayloadLength是否存在呢？<br>其实PositionDelta和Payload并不符合或然跟随规则，Payload是否存在，是由.fnm文件中对于每个域的配置中有关Payload的配置决定的(FieldOption.STORES_PAYLOADS) 。</p>
<p>当Payload不存在时，PayloadDelta本身不遵从或然跟随原则。</p>
<p>当Payload存在时，格式应该变成如下：Positions –&gt; &lt;PositionDelta,PayloadLength?,PayloadData&gt; Freq<br>从而PositionDelta和PayloadLength一起适用或然跟随规则。</p>
</blockquote>
<h3 id="4、跳表规则-Skip-list"><a href="#4、跳表规则-Skip-list" class="headerlink" title="4、跳表规则(Skip list)"></a>4、跳表规则(Skip list)</h3><p>为了提高查找的性能，Lucene在很多地方采取的跳表的数据结构。</p>
<p>跳表(Skip List)是如图的一种数据结构，有以下几个基本特征：</p>
<ul>
<li>元素是按顺序排列的，在Lucene中，或是按字典顺序排列，或是按从小到大顺序排列。</li>
<li>跳跃是有间隔的(Interval)，也即每次跳跃的元素数，间隔是事先配置好的，如图跳跃表的间隔为3。</li>
<li>跳跃表是由层次的(level)，每一层的每隔指定间隔的元素构成上一层，如图跳跃表共有2层。<br><img src="http://oawztil0a.bkt.clouddn.com/Blog/Lucene/Lucene%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6%E6%A0%BC%E5%BC%8F6.png" alt></li>
</ul>
<p>需要注意一点的是，在很多数据结构或算法书中都会有跳跃表的描述，原理都是大致相同的，但是定义稍有差别：</p>
<ul>
<li>对间隔(Interval)的定义： 如图中，有的认为间隔为2，即两个上层元素之间的元素数，不包括两个上层元素；有的认为是3，即两个上层元素之间的差，包括后面上层元素，不包括前面的上层元素；有的认为是4，即除两个上层元素之间的元素外，既包括前面，也包括后面的上层元素。Lucene是采取的第二种定义。</li>
<li>对层次(Level)的定义：如图中，有的认为应该包括原链表层，并从1开始计数，则总层次为3，为1，2，3层；有的认为应该包括原链表层，并从0计数，为0，1，2层；有的认为不应该包括原链表层，且从1开始计数，则为1，2层；有的认为不应该包括链表层，且从0开始计数，则为0，1层。Lucene采取的是最后一种定义。</li>
</ul>
<p>跳跃表比顺序查找，大大提高了查找速度，如查找元素72，原来要访问2，3，7，12，23，37，39，44，50，72总共10个元素，应用跳跃表后，只要首先访问第1层的50，发现72大于50，而第1层无下一个节点，然后访问第2层的94，发现94大于72，然后访问原链表的72，找到元素，共需要访问3个元素即可。</p>
<h2 id="四、具体格式"><a href="#四、具体格式" class="headerlink" title="四、具体格式"></a>四、具体格式</h2><p>上面交代了Lucene保存了正向信息和反向信息，Lucene还保存了一些其他信息。下面对这三种信息进行一一介绍。</p>
<h3 id="1、正向信息"><a href="#1、正向信息" class="headerlink" title="1、正向信息"></a>1、正向信息</h3><p>Index –&gt; Segments (segments.gen, segments_N) –&gt; Field(fnm, fdx, fdt) –&gt; Term (tvx, tvd, tvf) 上面的层次结构不是十分的准确,因为 segments.gen 和 segments_N 保存的是段(segment) 的元数据信息(metadata),其实是每个 Index 一个的,而段的真正的数据信息,是保存在域 (Field)和词(Term)中的。</p>
<h4 id="1-1、段的元数据信息-segments-N"><a href="#1-1、段的元数据信息-segments-N" class="headerlink" title="1.1、段的元数据信息(segments_N)"></a>1.1、段的元数据信息(segments_N)</h4><p>一个索引(Index)可以同时存在多个 segments_N(至于如何存在多个 segments_N,在描述完详 细信息之后会举例说明),然而当我们要打开一个索引的时候,我们必须要选择一个来打开, 那如何选择哪个 segments_N 呢?<br>Lucene 采取以下过程:</p>
<ol>
<li>在所有的 segments_N 中选择 N 最大的一个。基本逻辑参照 SegmentInfos.getCurrentSegmentGeneration(File[] files) , 其 基 本 思 路 就 是 在 所 有 以 segments 开头,并且不是 segments.gen 的文件中,选择 N 最大的一个作为 genA。</li>
<li><p>打开 segments.gen,其中保存了当前的 N 值。其格式如下,读出版本号(Version), 然后再读出两个 N,如果两者相等,则作为 genB。<br><img src="http://oawztil0a.bkt.clouddn.com/Blog/Lucene/Lucene%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6%E6%A0%BC%E5%BC%8F7.png" alt></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">IndexInput genInput = directory.openInput(IndexFileNames.SEGMENTS_GEN);<span class="comment">//"segments.gen" </span></span><br><span class="line"><span class="keyword">int</span> version = genInput.readInt();<span class="comment">//读出版本号 </span></span><br><span class="line"><span class="keyword">if</span> (version == FORMAT_LOCKLESS) &#123;<span class="comment">//如果版本号正确 </span></span><br><span class="line">    <span class="keyword">long</span> gen0 = genInput.readLong();<span class="comment">//读出第一个N </span></span><br><span class="line">    <span class="keyword">long</span> gen1 = genInput.readLong();<span class="comment">//读出第二个N </span></span><br><span class="line">    <span class="keyword">if</span> (gen0 == gen1) &#123;<span class="comment">//如果两者相等则为genB </span></span><br><span class="line">        genB = gen0; </span><br><span class="line">    &#125; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>在得到的genA和genB中选择最大的那个作为当前的N，方才打开segments_N文件。其基本逻辑如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> (genA &gt; genB) </span><br><span class="line">    gen = genA; </span><br><span class="line"><span class="keyword">else</span> </span><br><span class="line">    gen = genB;</span><br></pre></td></tr></table></figure>
</li>
</ol>
<p>如下图是segments_N的具体格式：<br><img src="http://oawztil0a.bkt.clouddn.com/Blog/Lucene/Lucene%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6%E6%A0%BC%E5%BC%8F8.png" alt></p>
<ul>
<li>Format：<ul>
<li>索引文件格式的版本号。</li>
<li>由于Lucene是在不断开发过程中的，因而不同版本的Lucene，其索引文件格式也不尽相同，于是规定一个版本号。</li>
<li>Lucene 2.1此值-3，Lucene 2.9时，此值为-9。</li>
<li>当用某个版本号的IndexReader读取另一个版本号生成的索引的时候，会因为此值不同而报错。</li>
</ul>
</li>
<li><p>Version：</p>
<ul>
<li>索引的版本号，记录了IndexWriter将修改提交到索引文件中的次数。</li>
<li>其初始值大多数情况下从索引文件里面读出，仅仅在索引开始创建的时候，被赋予当前的时间，已取得一个唯一值。</li>
<li>其值改变在IndexWriter.commit-&gt;IndexWriter.startCommit-&gt;SegmentInfos.prepareCommit-&gt;SegmentInfos.write-&gt;writeLong(++version)</li>
<li>其初始值之所最初取一个时间，是因为我们并不关心IndexWriter将修改提交到索引的具体次数，而更关心到底哪个是最新的。IndexReader中常比较自己的version和索引文件中的version是否相同来判断此IndexReader被打开后，还有没有被IndexWriter更新。<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//在DirectoryReader中有一下函数。</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">isCurrent</span><span class="params">()</span> <span class="keyword">throws</span> CorruptIndexException, IOException </span>&#123; </span><br><span class="line">  <span class="keyword">return</span> SegmentInfos.readCurrentVersion(directory) == segmentInfos.getVersion(); </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>NameCount</p>
<ul>
<li>是下一个新段(Segment)的段名。</li>
<li>所有属于同一个段的索引文件都以段名作为文件名，一般为_0.xxx, _0.yyy,  _1.xxx, _1.yyy ……</li>
<li>新生成的段的段名一般为原有最大段名加一。</li>
<li>如同的索引，NameCount读出来是2，说明新的段为_2.xxx, _2.yyy<br><img src="http://oawztil0a.bkt.clouddn.com/Blog/Lucene/Lucene%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6%E6%A0%BC%E5%BC%8F9.png" alt></li>
</ul>
</li>
<li><p>SegCount</p>
<ul>
<li>段(Segment)的个数。</li>
<li>如上图，此值为2。</li>
</ul>
</li>
<li>SegCount个段的元数据信息：<ul>
<li>SegName<ul>
<li>段名，所有属于同一个段的文件都有以段名作为文件名。</li>
<li>如上图，第一个段的段名为”_0”，第二个段的段名为”_1”</li>
</ul>
</li>
<li>SegSize<ul>
<li>此段中包含的文档数</li>
<li>然而此文档数是包括已经删除，又没有optimize的文档的，因为在optimize之前，Lucene的段中包含了所有被索引过的文档，而被删除的文档是保存在.del文件中的，在搜索的过程中，是先从段中读到了被删除的文档，然后再用.del中的标志，将这篇文档过滤掉。</li>
<li>如下的代码形成了上图的索引，可以看出索引了两篇文档形成了_0段，然后又删除了其中一篇，形成了_0_1.del，又索引了两篇文档形成_1段，然后又删除了其中一篇，形成_1_1.del。因而在两个段中，此值都是2。</li>
</ul>
</li>
</ul>
</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">IndexWriter writer = <span class="keyword">new</span> IndexWriter(FSDirectory.open(INDEX_DIR), <span class="keyword">new</span> StandardAnalyzer(Version.LUCENE_CURRENT), <span class="keyword">true</span>, IndexWriter.MaxFieldLength.LIMITED); </span><br><span class="line">writer.setUseCompoundFile(<span class="keyword">false</span>); </span><br><span class="line">indexDocs(writer, docDir);<span class="comment">//docDir中只有两篇文档</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//文档一为：Students should be allowed to go out with their friends, but not allowed to drink beer.</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//文档二为：My friend Jerry went to school to see his students but found them drunk which is not allowed.</span></span><br><span class="line"></span><br><span class="line">writer.commit();<span class="comment">//提交两篇文档，形成_0段。</span></span><br><span class="line"></span><br><span class="line">writer.deleteDocuments(<span class="keyword">new</span> Term(<span class="string">"contents"</span>, <span class="string">"school"</span>));<span class="comment">//删除文档二 </span></span><br><span class="line">writer.commit();<span class="comment">//提交删除，形成_0_1.del </span></span><br><span class="line">indexDocs(writer, docDir);<span class="comment">//再次索引两篇文档，Lucene不能判别文档与文档的不同，因而算两篇新的文档。 </span></span><br><span class="line">writer.commit();<span class="comment">//提交两篇文档，形成_1段 </span></span><br><span class="line">writer.deleteDocuments(<span class="keyword">new</span> Term(<span class="string">"contents"</span>, <span class="string">"school"</span>));<span class="comment">//删除第二次添加的文档二 </span></span><br><span class="line">writer.close();<span class="comment">//提交删除，形成_1_1.del</span></span><br></pre></td></tr></table></figure>
<ul>
<li><p>DelGen</p>
<ul>
<li>.del文件的版本号</li>
<li>Lucene中，在optimize之前，删除的文档是保存在.del文件中的。</li>
<li>在Lucene 2.9中，文档删除有以下几种方式：<ul>
<li>IndexReader.deleteDocument(int docID)是用IndexReader按文档号删除。</li>
<li>IndexReader.deleteDocuments(Term term)是用IndexReader删除包含此词(Term)的文档。</li>
<li>IndexWriter.deleteDocuments(Term term)是用IndexWriter删除包含此词(Term)的文档。</li>
<li>IndexWriter.deleteDocuments(Term[] terms)是用IndexWriter删除包含这些词(Term)的文档。</li>
<li>IndexWriter.deleteDocuments(Query query)是用IndexWriter删除能满足此查询(Query)的文档。</li>
<li>IndexWriter.deleteDocuments(Query[] queries)是用IndexWriter删除能满足这些查询(Query)的文档。</li>
<li>原来的版本中Lucene的删除一直是由IndexReader来完成的，在Lucene 2.9中虽可以用IndexWriter来删除，但是其实真正的实现是在IndexWriter中，保存了readerpool，当IndexWriter向索引文件提交删除的时候，仍然是从readerpool中得到相应的IndexReader，并用IndexReader来进行删除的。下面的代码可以说明：<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">IndexWriter.applyDeletes()</span><br><span class="line"></span><br><span class="line">-&gt; DocumentsWriter.applyDeletes(SegmentInfos)</span><br><span class="line"></span><br><span class="line">     -&gt; reader.deleteDocument(doc);</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
</ul>
</li>
<li><p>DelGen 是每当 IndexWriter 向索引文件中提交删除操作的时候,加 1,并生成 新的.del 文件。</p>
</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">IndexWriter.commit()</span><br><span class="line">-&gt; IndexWriter.applyDeletes()</span><br><span class="line">	-&gt; IndexWriter$ReaderPool.release(SegmentReader) </span><br><span class="line">		-&gt; SegmentReader(IndexReader).commit()</span><br><span class="line">			-&gt; SegmentReader.doCommit(Map) </span><br><span class="line">				-&gt; SegmentInfo.advanceDelGen()</span><br><span class="line">					-&gt; <span class="keyword">if</span> (delGen == NO) &#123; delGen = YES;</span><br><span class="line">						&#125; <span class="keyword">else</span> &#123; </span><br><span class="line">							delGen++;</span><br><span class="line">						&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">IndexWriter writer = <span class="keyword">new</span> IndexWriter(FSDirectory.open(INDEX_DIR), <span class="keyword">new</span> StandardAnalyzer(Version.LUCENE_CURRENT), <span class="keyword">true</span>, IndexWriter.MaxFieldLength.LIMITED); </span><br><span class="line">writer.setUseCompoundFile(<span class="keyword">false</span>);</span><br><span class="line"></span><br><span class="line">indexDocs(writer, docDir);<span class="comment">//索引两篇文档，一篇包含"school"，另一篇包含"beer" </span></span><br><span class="line">writer.commit();<span class="comment">//提交两篇文档到索引文件，形成段(Segment) "_0" </span></span><br><span class="line">writer.deleteDocuments(<span class="keyword">new</span> Term(<span class="string">"contents"</span>, <span class="string">"school"</span>));<span class="comment">//删除包含"school"的文档，其实是删除了两篇文档中的一篇。 </span></span><br><span class="line">writer.commit();<span class="comment">//提交删除到索引文件，形成"_0_1.del" </span></span><br><span class="line">writer.deleteDocuments(<span class="keyword">new</span> Term(<span class="string">"contents"</span>, <span class="string">"beer"</span>));<span class="comment">//删除包含"beer"的文档，其实是删除了两篇文档中的另一篇。 </span></span><br><span class="line">writer.commit();<span class="comment">//提交删除到索引文件，形成"_0_2.del" </span></span><br><span class="line">indexDocs(writer, docDir);<span class="comment">//索引两篇文档，和上次的文档相同，但是Lucene无法区分，认为是另外两篇文档。 </span></span><br><span class="line">writer.commit();<span class="comment">//提交两篇文档到索引文件，形成段"_1" </span></span><br><span class="line">writer.deleteDocuments(<span class="keyword">new</span> Term(<span class="string">"contents"</span>, <span class="string">"beer"</span>));<span class="comment">//删除包含"beer"的文档，其中段"_0"已经无可删除，段"_1"被删除一篇。 </span></span><br><span class="line">writer.close();<span class="comment">//提交删除到索引文件，形成"_1_1.del"</span></span><br></pre></td></tr></table></figure>
<p>形成的索引文件如下：<br><img src="http://oawztil0a.bkt.clouddn.com/Blog/Lucene/Lucene%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6%E6%A0%BC%E5%BC%8F10.png" alt></p>
<ul>
<li>DocStoreOffset</li>
<li>DocStoreSegment</li>
<li>DocStoreIsCompoundFile<ul>
<li>对于域(Stored Field)和词向量(Term Vector)的存储可以有不同的方式，即可以每个段(Segment)单独存储自己的域和词向量信息，也可以多个段共享域和词向量，把它们存储到一个段中去。</li>
<li>如果DocStoreOffset为-1，则此段单独存储自己的域和词向量，从存储文件上来看，如果此段段名为XXX，则此段有自己的XXX.fdt，XXX.fdx，XXX.tvf，XXX.tvd，XXX.tvx文件。DocStoreSegment和DocStoreIsCompoundFile在此处不被保存。</li>
<li>如果DocStoreOffset不为-1，则DocStoreSegment保存了共享的段的名字，比如为YYY，DocStoreOffset则为此段的域及词向量信息在共享段中的偏移量。则此段没有自己的XXX.fdt，XXX.fdx，XXX.tvf，XXX.tvd，XXX.tvx文件，而是将信息存放在共享段的YYY.fdt，YYY.fdx，YYY.tvf，YYY.tvd，YYY.tvx文件中。</li>
<li>DocumentsWriter中有两个成员变量：String segment是当前索引信息存放的段，String docStoreSegment是域和词向量信息存储的段。两者可以相同也可以不同，决定了域和词向量信息是存储在本段中，还是和其他的段共享。</li>
<li>IndexWriter.flush(boolean triggerMerge, boolean flushDocStores, boolean flushDeletes)中第二个参数flushDocStores会影响到是否单独或是共享存储。其实最终影响的是DocumentsWriter.closeDocStore()。每当flushDocStores为false时，closeDocStore不被调用，说明下次添加到索引文件中的域和词向量信息是同此次共享一个段的。直到flushDocStores为true的时候，closeDocStore被调用，从而下次添加到索引文件中的域和词向量信息将被保存在一个新的段中，不同此次共享一个段(在这里需要指出的是Lucene的一个很奇怪的实现，虽然下次域和词向量信息是被保存到新的段中，然而段名却是这次被确定了的，在initSegmentName中当docStoreSegment == null时，被置为当前的segment，而非下一个新的segment，docStoreSegment = segment，于是会出现如下面的例子的现象)。</li>
<li>好在共享域和词向量存储并不是经常被使用到，实现也或有缺陷，暂且解释到此。</li>
</ul>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">IndexWriter writer = new IndexWriter(FSDirectory.open(INDEX_DIR), new StandardAnalyzer(Version.LUCENE_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED); writer.setUseCompoundFile(false);</span><br><span class="line"></span><br><span class="line">indexDocs(writer, docDir); </span><br><span class="line">writer.flush();</span><br><span class="line"></span><br><span class="line">//flush生成segment &quot;_0&quot;，并且flush函数中，flushDocStores设为false，也即下个段将同本段共享域和词向量信息，这时DocumentsWriter中的docStoreSegment= &quot;_0&quot;。</span><br><span class="line">indexDocs(writer, docDir); </span><br><span class="line">writer.commit();</span><br><span class="line"></span><br><span class="line">//commit生成segment &quot;_1&quot;，由于上次flushDocStores设为false，于是段&quot;_1&quot;的域以及词向量信息是保存在&quot;_0&quot;中的，在这个时刻，段&quot;_1&quot;并不生成自己的&quot;_1.fdx&quot;和&quot;_1.fdt&quot;。然而在commit函数中，flushDocStores设为true，也即下个段将单独使用新的段来存储域和词向量信息。然而这时，DocumentsWriter中的docStoreSegment= &quot;_1&quot;，也即当段&quot;_2&quot;存储其域和词向量信息的时候，是存在&quot;_1.fdx&quot;和&quot;_1.fdt&quot;中的，而段&quot;_1&quot;的域和词向量信息却是存在&quot;_0.fdt&quot;和&quot;_0.fdx&quot;中的，这一点非常令人困惑。</span><br></pre></td></tr></table></figure>
<blockquote>
<p>如图writer.commit的时候，_1.fdt和_1.fdx并没有形成。<br><img src="http://oawztil0a.bkt.clouddn.com/Blog/Lucene/Lucene%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6%E6%A0%BC%E5%BC%8F11.png" alt><br>indexDocs(writer, docDir);<br>     writer.flush();<br>//段”_2”形成，由于上次flushDocStores设为true，其域和词向量信息是新创建一个段保存的，却是保存在_1.fdt和_1.fdx中的，这时候才产生了此二文件。<br><img src="http://oawztil0a.bkt.clouddn.com/Blog/Lucene/Lucene%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6%E6%A0%BC%E5%BC%8F12.png" alt><br>indexDocs(writer, docDir);<br>     writer.flush();<br>//段”_3”形成，由于上次flushDocStores设为false，其域和词向量信息是共享一个段保存的，也是是保存在_1.fdt和_1.fdx中的</p>
<pre><code>indexDocs(writer, docDir); 
     writer.commit();
</code></pre><p>//段”_4”形成，由于上次flushDocStores设为false，其域和词向量信息是共享一个段保存的，也是是保存在_1.fdt和_1.fdx中的。然而函数commit中flushDocStores设为true，也意味着下一个段将新创建一个段保存域和词向量信息，此时DocumentsWriter中docStoreSegment= “_4”，也表明了虽然段”_4”的域和词向量信息保存在了段”_1”中，将来的域和词向量信息却要保存在段”_4”中。此时”_4.fdx”和”_4.fdt”尚未产生。<br><img src="http://oawztil0a.bkt.clouddn.com/Blog/Lucene/Lucene%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6%E6%A0%BC%E5%BC%8F13.png" alt><br>    indexDocs(writer, docDir);<br>     writer.flush();<br>//段”_5”形成，由于上次flushDocStores设为true，其域和词向量信息是新创建一个段保存的，却是保存在_4.fdt和_4.fdx中的，这时候才产生了此二文件。<br><img src="http://oawztil0a.bkt.clouddn.com/Blog/Lucene/Lucene%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6%E6%A0%BC%E5%BC%8F14.png" alt><br> indexDocs(writer, docDir);<br>     writer.commit();<br>     writer.close();<br>//段”_6”形成，由于上次flushDocStores设为false，其域和词向量信息是共享一个段保存的，也是是保存在_4.fdt和_4.fdx中的<br><img src="http://oawztil0a.bkt.clouddn.com/Blog/Lucene/Lucene%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6%E6%A0%BC%E5%BC%8F15.png" alt></p>
</blockquote>
<ul>
<li><ul>
<li>HasSingleNormFile<ul>
<li>在搜索的过程中，标准化因子(Normalization Factor)会影响文档最后的评分。</li>
<li>不同的文档重要性不同，不同的域重要性也不同。因而每个文档的每个域都可以有自己的标准化因子。</li>
<li>如果HasSingleNormFile为1，则所有的标准化因子都是存在.nrm文件中的。</li>
<li>如果HasSingleNormFile不是1，则每个域都有自己的标准化因子文件.fN</li>
</ul>
</li>
<li>NumField<ul>
<li>域的数量</li>
</ul>
</li>
<li>NormGen<ul>
<li>如果每个域有自己的标准化因子文件，则此数组描述了每个标准化因子文件的版本号，也即.fN的N。</li>
</ul>
</li>
<li>IsCompoundFile<ul>
<li>是否保存为复合文件，也即把同一个段中的文件按照一定格式，保存在一个文件当中，这样可以减少每次打开文件的个数。</li>
<li>是否为复合文件，由接口IndexWriter.setUseCompoundFile(boolean)设定。 </li>
<li>非符合文件同符合文件的对比如下图：</li>
<li><img src="http://oawztil0a.bkt.clouddn.com/Blog/Lucene/Lucene%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6%E6%A0%BC%E5%BC%8F16.png" alt></li>
</ul>
</li>
<li>DeletionCount<ul>
<li>记录了此段中删除的文档的数目。</li>
</ul>
</li>
<li>HasProx<ul>
<li>如果至少有一个段omitTf为false，也即词频(term freqency)需要被保存，则HasProx为1，否则为0。</li>
</ul>
</li>
</ul>
</li>
<li>Diagnostics<ul>
<li>调试信息。</li>
</ul>
</li>
<li>User map data<ul>
<li>保存了用户从字符串到字符串的映射Map</li>
</ul>
</li>
<li>CheckSum<ul>
<li>此文件segment_N的校验和。</li>
</ul>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">读取此文件格式参考SegmentInfos.read(Directory directory, String segmentFileName):</span><br><span class="line"></span><br><span class="line">int format = input.readInt();</span><br><span class="line">version = input.readLong(); // read version</span><br><span class="line">counter = input.readInt(); // read counter</span><br><span class="line">for (int i = input.readInt(); i &gt; 0; i--) // read segmentInfos</span><br><span class="line">	add(new SegmentInfo(directory, format, input));</span><br><span class="line">	name = input.readString();</span><br><span class="line">	docCount = input.readInt();</span><br><span class="line">	delGen = input.readLong();</span><br><span class="line">	docStoreOffset = input.readInt();</span><br><span class="line">	docStoreSegment = input.readString();</span><br><span class="line">	docStoreIsCompoundFile = (1 == input.readByte());</span><br><span class="line">	hasSingleNormFile = (1 == input.readByte());</span><br><span class="line">	int numNormGen = input.readInt();</span><br><span class="line">	normGen = new long[numNormGen];</span><br><span class="line">	for(int j=0;j</span><br><span class="line">		normGen[j] = input.readLong();</span><br><span class="line">	isCompoundFile = input.readByte();</span><br><span class="line">	delCount = input.readInt();</span><br><span class="line">	hasProx = input.readByte() == 1;</span><br><span class="line">	diagnostics = input.readStringStringMap();</span><br><span class="line">userData = input.readStringStringMap();</span><br><span class="line">final long checksumNow = input.getChecksum();</span><br><span class="line">final long checksumThen = input.readLong();</span><br></pre></td></tr></table></figure>
<h4 id="1-2、域-Field-的元数据信息-fnm"><a href="#1-2、域-Field-的元数据信息-fnm" class="headerlink" title="1.2、域(Field)的元数据信息(.fnm)"></a>1.2、域(Field)的元数据信息(.fnm)</h4><p>一个段(Segment)包含多个域，每个域都有一些元数据信息，保存在.fnm文件中，.fnm文件的格式如下：<br><img src="http://oawztil0a.bkt.clouddn.com/Blog/Lucene/Lucene%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6%E6%A0%BC%E5%BC%8F17.png" alt></p>
<ul>
<li>FNMVersion<ul>
<li>是fnm文件的版本号，对于Lucene 2.9为-2</li>
</ul>
</li>
<li>FieldsCount<ul>
<li>域的数目</li>
</ul>
</li>
<li>一个数组的域(Fields)<ul>
<li>FieldName：域名，如”title”，”modified”，”content”等。</li>
<li>FieldBits:一系列标志位，表明对此域的索引方式<ul>
<li>最低位：1表示此域被索引，0则不被索引。所谓被索引，也即放到倒排表中去。<ul>
<li>仅仅被索引的域才能够被搜到。</li>
<li>Field.Index.NO则表示不被索引。</li>
<li>Field.Index.ANALYZED则表示不但被索引，而且被分词，比如索引”hello world”后，无论是搜”hello”，还是搜”world”都能够被搜到。</li>
<li>Field.Index.NOT_ANALYZED表示虽然被索引，但是不分词，比如索引”hello world”后，仅当搜”hello world”时，能够搜到，搜”hello”和搜”world”都搜不到。</li>
<li>一个域出了能够被索引，还能够被存储，仅仅被存储的域是搜索不到的，但是能通过文档号查到，多用于不想被搜索到，但是在通过其它域能够搜索到的情况下，能够随着文档号返回给用户的域。</li>
<li>Field.Store.Yes则表示存储此域，Field.Store.NO则表示不存储此域。</li>
</ul>
</li>
<li>倒数第二位：1表示保存词向量，0为不保存词向量。<ul>
<li>Field.TermVector.YES表示保存词向量。</li>
<li>Field.TermVector.NO表示不保存词向量。</li>
</ul>
</li>
<li>倒数第三位：1表示在词向量中保存位置信息。<ul>
<li>Field.TermVector.WITH_POSITIONS</li>
</ul>
</li>
<li>倒数第四位：1表示在词向量中保存偏移量信息。<ul>
<li>Field.TermVector.WITH_OFFSETS</li>
</ul>
</li>
<li>倒数第五位：1表示不保存标准化因子<ul>
<li>Field.Index.ANALYZED_NO_NORMS</li>
<li>Field.Index.NOT_ANALYZED_NO_NORMS</li>
</ul>
</li>
<li>倒数第六位：是否保存payload</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>要了解域的元数据信息，还要了解以下几点：</p>
<ul>
<li>位置(Position)和偏移量(Offset)的区别<ul>
<li>位置是基于词Term的，偏移量是基于字母或汉字的。<br><img src="http://oawztil0a.bkt.clouddn.com/Blog/Lucene/Lucene%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6%E6%A0%BC%E5%BC%8F18.png" alt></li>
</ul>
</li>
<li>索引域(Indexed)和存储域(Stored)的区别<ul>
<li>一个域为什么会被存储(store)而不被索引(Index)呢？在一个文档中的所有信息中，有这样一部分信息，可能不想被索引从而可以搜索到，但是当这个文档由于其他的信息被搜索到时，可以同其他信息一同返回。</li>
<li>举个例子，读研究生时，您好不容易写了一篇论文交给您的导师，您的导师却要他所第一作者而您做第二作者，然而您导师不想别人在论文系统中搜索您的名字时找到这篇论文，于是在论文系统中，把第二作者这个Field的Indexed设为false，这样别人搜索您的名字，永远不知道您写过这篇论文，只有在别人搜索您导师的名字从而找到您的文章时，在一个角落表述着第二作者是您。</li>
</ul>
</li>
<li>payload的使用<ul>
<li>我们知道，索引是以倒排表形式存储的，对于每一个词，都保存了包含这个词的一个链表，当然为了加快查询速度，此链表多用跳跃表进行存储。</li>
<li>Payload信息就是存储在倒排表中的，同文档号一起存放，多用于存储与每篇文档相关的一些信息。当然这部分信息也可以存储域里(stored Field)，两者从功能上基本是一样的，然而当要存储的信息很多的时候，存放在倒排表里，利用跳跃表，有利于大大提高搜索速度。</li>
<li>Payload的存储方式如下图：<img src="http://oawztil0a.bkt.clouddn.com/Blog/Lucene/Lucene%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6%E6%A0%BC%E5%BC%8F19.png" alt></li>
<li>Payload主要有以下几种用法：<ul>
<li>存储每个文档都有的信息：比如有的时候，我们想给每个文档赋一个我们自己的文档号，而不是用Lucene自己的文档号。于是我们可以声明一个特殊的域(Field)”_ID”和特殊的词(Term)”_ID”，使得每篇文档都包含词”_ID”，于是在词”_ID”的倒排表里面对于每篇文档又有一项，每一项都有一个payload，于是我们可以在payload里面保存我们自己的文档号。每当我们得到一个Lucene的文档号的时候，就能从跳跃表中查找到我们自己的文档号。</li>
</ul>
</li>
</ul>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line">//声明一个特殊的域和特殊的词 </span><br><span class="line">public static final String ID_PAYLOAD_FIELD = &quot;_ID&quot;;</span><br><span class="line"></span><br><span class="line">public static final String ID_PAYLOAD_TERM = &quot;_ID&quot;;</span><br><span class="line"></span><br><span class="line">public static final Term ID_TERM = new Term(ID_PAYLOAD_TERM, ID_PAYLOAD_FIELD);</span><br><span class="line"></span><br><span class="line">//声明一个特殊的TokenStream，它只生成一个词(Term)，就是那个特殊的词，在特殊的域里面。</span><br><span class="line"></span><br><span class="line">static class SinglePayloadTokenStream extends TokenStream &#123; </span><br><span class="line">    private Token token; </span><br><span class="line">    private boolean returnToken = false;</span><br><span class="line"></span><br><span class="line">    SinglePayloadTokenStream(String idPayloadTerm) &#123; </span><br><span class="line">        char[] term = idPayloadTerm.toCharArray(); </span><br><span class="line">        token = new Token(term, 0, term.length, 0, term.length); </span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    void setPayloadValue(byte[] value) &#123; </span><br><span class="line">        token.setPayload(new Payload(value)); </span><br><span class="line">        returnToken = true; </span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public Token next() throws IOException &#123; </span><br><span class="line">        if (returnToken) &#123; </span><br><span class="line">            returnToken = false; </span><br><span class="line">            return token; </span><br><span class="line">        &#125; else &#123; </span><br><span class="line">            return null; </span><br><span class="line">        &#125; </span><br><span class="line">    &#125; </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">//对于每一篇文档，都让它包含这个特殊的词，在特殊的域里面</span><br><span class="line"></span><br><span class="line">SinglePayloadTokenStream singlePayloadTokenStream = new SinglePayloadTokenStream(ID_PAYLOAD_TERM); </span><br><span class="line">singlePayloadTokenStream.setPayloadValue(long2bytes(id)); </span><br><span class="line">doc.add(new Field(ID_PAYLOAD_FIELD, singlePayloadTokenStream));</span><br><span class="line"></span><br><span class="line">//每当得到一个Lucene的文档号时，通过以下的方式得到payload里面的文档号 </span><br><span class="line">long id = 0; </span><br><span class="line">TermPositions tp = reader.termPositions(ID_PAYLOAD_TERM); </span><br><span class="line">boolean ret = tp.skipTo(docID); </span><br><span class="line">tp.nextPosition(); </span><br><span class="line">int payloadlength = tp.getPayloadLength(); </span><br><span class="line">byte[] payloadBuffer = new byte[payloadlength]; </span><br><span class="line">tp.getPayload(payloadBuffer, 0); </span><br><span class="line">id = bytes2long(payloadBuffer); </span><br><span class="line">tp.close();</span><br></pre></td></tr></table></figure>
<ul>
<li><ul>
<li><ul>
<li>影响词的评分<ul>
<li>在Similarity抽象类中有函数public float scorePayload(byte [] payload, int offset, int length)  可以根据payload的值影响评分。</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>读取域元数据信息的代码如下：</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">FieldInfos.read(IndexInput, String)</span><br><span class="line"></span><br><span class="line">	int firstInt = input.readVInt();</span><br><span class="line">	size = input.readVInt();</span><br><span class="line">	for (int i = 0; i &lt; size; i++)</span><br><span class="line">		String name = input.readString();</span><br><span class="line">		byte bits = input.readByte();</span><br><span class="line">		boolean isIndexed = (bits &amp; IS_INDEXED) != 0;</span><br><span class="line">		boolean storeTermVector = (bits &amp; STORE_TERMVECTOR) != 0;</span><br><span class="line">		boolean storePositionsWithTermVector = (bits &amp;STORE_POSITIONS_WITH_TERMVECTOR) != 0;</span><br><span class="line">		boolean storeOffsetWithTermVector = (bits &amp; STORE_OFFSET_WITH_TERMVECTOR) != 0;</span><br><span class="line">		boolean omitNorms = (bits &amp; OMIT_NORMS) != 0;</span><br><span class="line">		boolean storePayloads = (bits &amp; STORE_PAYLOADS) != 0;</span><br><span class="line">		boolean omitTermFreqAndPositions = (bits &amp; OMIT_TERM_FREQ_AND_POSITIONS) != 0;</span><br></pre></td></tr></table></figure>
<h4 id="1-3、域-Field-的数据信息-fdt，-fdx"><a href="#1-3、域-Field-的数据信息-fdt，-fdx" class="headerlink" title="1.3、域(Field)的数据信息(.fdt，.fdx)"></a>1.3、域(Field)的数据信息(.fdt，.fdx)</h4><p><img src="http://oawztil0a.bkt.clouddn.com/Blog/Lucene/Lucene%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6%E6%A0%BC%E5%BC%8F20.png" alt></p>
<ul>
<li>域数据文件(fdt):<ul>
<li>真正保存存储域(stored field)信息的是fdt文件</li>
<li>在一个段(segment)中总共有segment size篇文档，所以fdt文件中共有segment size个项，每一项保存一篇文档的域的信息</li>
<li>对于每一篇文档，一开始是一个fieldcount，也即此文档包含的域的数目，接下来是fieldcount个项，每一项保存一个域的信息。</li>
<li>对于每一个域，fieldnum是域号，接着是一个8位的byte，最低一位表示此域是否分词(tokenized)，倒数第二位表示此域是保存字符串数据还是二进制数据，倒数第三位表示此域是否被压缩，再接下来就是存储域的值，比如new Field(“title”, “lucene in action”, Field.Store.Yes, …)，则此处存放的就是”lucene in action”这个字符串。</li>
</ul>
</li>
<li>域索引文件(fdx)<ul>
<li>由域数据文件格式我们知道，每篇文档包含的域的个数，每个存储域的值都是不一样的，因而域数据文件中segment size篇文档，每篇文档占用的大小也是不一样的，那么如何在fdt中辨别每一篇文档的起始地址和终止地址呢，如何能够更快的找到第n篇文档的存储域的信息呢？就是要借助域索引文件。</li>
<li>域索引文件也总共有segment size个项，每篇文档都有一个项，每一项都是一个long，大小固定，每一项都是对应的文档在fdt文件中的起始地址的偏移量，这样如果我们想找到第n篇文档的存储域的信息，只要在fdx中找到第n项，然后按照取出的long作为偏移量，就可以在fdt文件中找到对应的存储域的信息。</li>
</ul>
</li>
<li>读取域数据信息的代码如下：</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">Document FieldsReader.doc(int n, FieldSelector fieldSelector)</span><br><span class="line"></span><br><span class="line">	long position = indexStream.readLong();//indexStream points to &quot;.fdx&quot;</span><br><span class="line">	fieldsStream.seek(position);//fieldsStream points to &quot;fdt&quot;</span><br><span class="line">	int numFields = fieldsStream.readVInt();</span><br><span class="line">	for (int i = 0; i &lt; numFields; i++)</span><br><span class="line">		int fieldNumber = fieldsStream.readVInt();</span><br><span class="line">		byte bits = fieldsStream.readByte();</span><br><span class="line">		boolean compressed = (bits &amp; FieldsWriter.FIELD_IS_COMPRESSED) != 0;</span><br><span class="line">		boolean tokenize = (bits &amp; FieldsWriter.FIELD_IS_TOKENIZED) != 0;</span><br><span class="line">		boolean binary = (bits &amp; FieldsWriter.FIELD_IS_BINARY) != 0;</span><br><span class="line">		if (binary)</span><br><span class="line">			int toRead = fieldsStream.readVInt();</span><br><span class="line">			final byte[] b = new byte[toRead];</span><br><span class="line">			fieldsStream.readBytes(b, 0, b.length);</span><br><span class="line">			if (compressed)</span><br><span class="line">				int toRead = fieldsStream.readVInt();</span><br><span class="line">				final byte[] b = new byte[toRead];</span><br><span class="line">				fieldsStream.readBytes(b, 0, b.length);</span><br><span class="line">				uncompress(b),</span><br><span class="line">		else</span><br><span class="line">			fieldsStream.readString()</span><br></pre></td></tr></table></figure>
<h4 id="1-4、词向量-Term-Vector-的数据信息-tvx，-tvd，-tvf"><a href="#1-4、词向量-Term-Vector-的数据信息-tvx，-tvd，-tvf" class="headerlink" title="1.4、词向量(Term Vector)的数据信息(.tvx，.tvd，.tvf)"></a>1.4、词向量(Term Vector)的数据信息(.tvx，.tvd，.tvf)</h4><p><img src="http://oawztil0a.bkt.clouddn.com/Blog/Lucene/Lucene%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6%E6%A0%BC%E5%BC%8F21.png" alt><br>词向量信息是从索引(index)到文档(document)到域(field)到词(term)的正向信息，有了词向量信息，我们就可以得到一篇文档包含那些词的信息。</p>
<ul>
<li>词向量索引文件(tvx)<ul>
<li>一个段(segment)包含N篇文档，此文件就有N项，每一项代表一篇文档。</li>
<li>每一项包含两部分信息：第一部分是词向量文档文件(tvd)中此文档的偏移量，第二部分是词向量域文件(tvf)中此文档的第一个域的偏移量。</li>
</ul>
</li>
<li>词向量文档文件(tvd)<ul>
<li>一个段(segment)包含N篇文档，此文件就有N项，每一项包含了此文档的所有的域的信息。</li>
<li>每一项首先是此文档包含的域的个数NumFields，然后是一个NumFields大小的数组，数组的每一项是域号。然后是一个(NumFields - 1)大小的数组，由前面我们知道，每篇文档的第一个域在tvf中的偏移量在tvx文件中保存，而其他(NumFields - 1)个域在tvf中的偏移量就是第一个域的偏移量加上这(NumFields - 1)个数组的每一项的值。</li>
</ul>
</li>
<li>词向量域文件(tvf)<ul>
<li>此文件包含了此段中的所有的域，并不对文档做区分，到底第几个域到第几个域是属于那篇文档，是由tvx中的第一个域的偏移量以及tvd中的(NumFields - 1)个域的偏移量来决定的。</li>
<li>对于每一个域，首先是此域包含的词的个数NumTerms，然后是一个8位的byte，最后一位是指定是否保存位置信息，倒数第二位是指定是否保存偏移量信息。然后是NumTerms个项的数组，每一项代表一个词(Term)，对于每一个词，由词的文本TermText，词频TermFreq(也即此词在此文档中出现的次数)，词的位置信息，词的偏移量信息。</li>
</ul>
</li>
<li>读取词向量数据信息的代码如下：</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">TermVectorsReader.get(int docNum, String field, TermVectorMapper)</span><br><span class="line"></span><br><span class="line">	int fieldNumber = fieldInfos.fieldNumber(field);//通过field名字得到field号</span><br><span class="line">	seekTvx(docNum);//在tvx文件中按docNum文档号找到相应文档的项</span><br><span class="line">	long tvdPosition = tvx.readLong();//找到tvd文件中相应文档的偏移量</span><br><span class="line">	tvd.seek(tvdPosition);//在tvd文件中按偏移量找到相应文档的项</span><br><span class="line">	int fieldCount = tvd.readVInt();//此文档包含的域的个数。</span><br><span class="line">	for (int i = 0; i &lt; fieldCount; i++) //按域号查找域</span><br><span class="line">		number = tvd.readVInt();</span><br><span class="line">		if (number == fieldNumber)</span><br><span class="line">			found = i;</span><br><span class="line">	position = tvx.readLong();//在tvx中读出此文档的第一个域在tvf中的偏移量</span><br><span class="line">	for (int i = 1; i &lt;= found; i++)</span><br><span class="line">		position += tvd.readVLong();//加上所要找的域在tvf中的偏移量</span><br><span class="line">	tvf.seek(position);</span><br><span class="line">	int numTerms = tvf.readVInt();</span><br><span class="line">	byte bits = tvf.readByte();</span><br><span class="line">	storePositions = (bits &amp; STORE_POSITIONS_WITH_TERMVECTOR) != 0;</span><br><span class="line">	storeOffsets = (bits &amp; STORE_OFFSET_WITH_TERMVECTOR) != 0;</span><br><span class="line">	for (int i = 0; i &lt; numTerms; i++)</span><br><span class="line">		start = tvf.readVInt();</span><br><span class="line">		deltaLength = tvf.readVInt();</span><br><span class="line">		totalLength = start + deltaLength;</span><br><span class="line">		tvf.readBytes(byteBuffer, start, deltaLength);</span><br><span class="line">		term = new String(byteBuffer, 0, totalLength, &quot;UTF-8&quot;);</span><br><span class="line">		if (storePositions)</span><br><span class="line">			positions = new int[freq];</span><br><span class="line">			int prevPosition = 0;</span><br><span class="line">			for (int j = 0; j &lt; freq; j++)</span><br><span class="line">				positions[j] = prevPosition + tvf.readVInt();</span><br><span class="line">				prevPosition = positions[j];</span><br><span class="line">		if (storeOffsets)</span><br><span class="line">			offsets = new TermVectorOffsetInfo[freq];</span><br><span class="line">			int prevOffset = 0;</span><br><span class="line">			for (int j = 0; j &lt; freq; j++)</span><br><span class="line">			int startOffset = prevOffset + tvf.readVInt();</span><br><span class="line">			int endOffset = startOffset + tvf.readVInt();</span><br><span class="line">			offsets[j] = new TermVectorOffsetInfo(startOffset, endOffset);</span><br><span class="line">			prevOffset = endOffset;</span><br></pre></td></tr></table></figure>
<h3 id="2、反向信息"><a href="#2、反向信息" class="headerlink" title="2、反向信息"></a>2、反向信息</h3><p>反向信息是索引文件的核心，也即反向索引。</p>
<p>反向索引包括两部分，左面是词典(Term Dictionary)，右面是倒排表(Posting List)。</p>
<p>在Lucene中，这两部分是分文件存储的，词典是存储在tii，tis中的，倒排表又包括两部分，一部分是文档号及词频，保存在frq中，一部分是词的位置信息，保存在prx中。</p>
<ul>
<li>Term Dictionary (tii, tis)<ul>
<li>–&gt; Frequencies (.frq)</li>
<li>–&gt; Positions (.prx)</li>
</ul>
</li>
</ul>
<h4 id="2-1、词典-tis-及词典索引-tii-信息"><a href="#2-1、词典-tis-及词典索引-tii-信息" class="headerlink" title="2.1、词典(tis)及词典索引(tii)信息"></a>2.1、词典(tis)及词典索引(tii)信息</h4><p><img src="http://oawztil0a.bkt.clouddn.com/Blog/Lucene/Lucene%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6%E6%A0%BC%E5%BC%8F22.png" alt><br>在词典中，所有的词是按照字典顺序排序的。</p>
<ul>
<li>词典文件(tis)<ul>
<li>TermCount：词典中包含的总的词数</li>
<li>IndexInterval：为了加快对词的查找速度，也应用类似跳跃表的结构，假设IndexInterval为4，则在词典索引(tii)文件中保存第4个，第8个，第12个词，这样可以加快在词典文件中查找词的速度。</li>
<li>SkipInterval：倒排表无论是文档号及词频，还是位置信息，都是以跳跃表的结构存在的，SkipInterval是跳跃的步数。</li>
<li>MaxSkipLevels：跳跃表是多层的，这个值指的是跳跃表的最大层数。</li>
<li>TermCount个项的数组，每一项代表一个词，对于每一个词，以前缀后缀规则存放词的文本信息(PrefixLength + Suffix)，词属于的域的域号(FieldNum)，有多少篇文档包含此词(DocFreq)，此词的倒排表在frq，prx中的偏移量(FreqDelta, ProxDelta)，此词的倒排表的跳跃表在frq中的偏移量(SkipDelta)，这里之所以用Delta，是应用差值规则。</li>
</ul>
</li>
<li><p>词典索引文件(tii)</p>
<ul>
<li>词典索引文件是为了加快对词典文件中词的查找速度，保存每隔IndexInterval个词。</li>
<li>词典索引文件是会被全部加载到内存中去的。</li>
<li>IndexTermCount = TermCount / IndexInterval：词典索引文件中包含的词数。</li>
<li>IndexInterval同词典文件中的IndexInterval。</li>
<li>SkipInterval同词典文件中的SkipInterval。</li>
<li>MaxSkipLevels同词典文件中的MaxSkipLevels。</li>
<li>IndexTermCount个项的数组，每一项代表一个词，每一项包括两部分，第一部分是词本身(TermInfo)，第二部分是在词典文件中的偏移量(IndexDelta)。假设IndexInterval为4，此数组中保存第4个，第8个，第12个词。。。</li>
</ul>
</li>
<li><p>读取词典及词典索引文件的代码如下：</p>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">origEnum = new SegmentTermEnum(directory.openInput(segment + &quot;.&quot; + IndexFileNames.TERMS_EXTENSION,readBufferSize), fieldInfos, false);//用于读取tis文件</span><br><span class="line"></span><br><span class="line">	int firstInt = input.readInt();</span><br><span class="line">	size = input.readLong();</span><br><span class="line">	indexInterval = input.readInt();</span><br><span class="line">	skipInterval = input.readInt();</span><br><span class="line">	maxSkipLevels = input.readInt();</span><br><span class="line"></span><br><span class="line">SegmentTermEnum indexEnum = new SegmentTermEnum(directory.openInput(segment + &quot;.&quot; + IndexFileNames.TERMS_INDEX_EXTENSION, readBufferSize), fieldInfos, true);//用于读取tii文件</span><br><span class="line"></span><br><span class="line">	indexTerms = new Term[indexSize];</span><br><span class="line">	indexInfos = new TermInfo[indexSize];</span><br><span class="line">	indexPointers = new long[indexSize];</span><br><span class="line">	for (int i = 0; indexEnum.next(); i++)</span><br><span class="line">		indexTerms[i] = indexEnum.term();</span><br><span class="line">		indexInfos[i] = indexEnum.termInfo();</span><br><span class="line">		indexPointers[i] = indexEnum.indexPointer;</span><br></pre></td></tr></table></figure>
<h4 id="2-2、文档号及词频-frq-信息"><a href="#2-2、文档号及词频-frq-信息" class="headerlink" title="2.2、文档号及词频(frq)信息"></a>2.2、文档号及词频(frq)信息</h4><p><img src="http://oawztil0a.bkt.clouddn.com/Blog/Lucene/Lucene%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6%E6%A0%BC%E5%BC%8F23.png" alt></p>
<p>文档号及词频文件里面保存的是倒排表，是以跳跃表形式存在的。</p>
<ul>
<li>此文件包含TermCount个项，每一个词都有一项，因为每一个词都有自己的倒排表。</li>
<li>对于每一个词的倒排表都包括两部分，一部分是倒排表本身，也即一个数组的文档号及词频，另一部分是跳跃表，为了更快的访问和定位倒排表中文档号及词频的位置。</li>
<li>对于文档号和词频的存储应用的是差值规则和或然跟随规则，Lucene的文档本身有以下几句话，比较难以理解，在此解释一下：</li>
</ul>
<blockquote>
<p>For example, the TermFreqs for a term which occurs once in document seven and three times in document eleven, with omitTf false, would be the following sequence of VInts:<br>15, 8, 3<br>If omitTf were true it would be this sequence of VInts instead:<br>7,4</p>
<p>首先我们看omitTf=false的情况，也即我们在索引中会存储一个文档中term出现的次数。</p>
<p>例子中说了，表示在文档7中出现1次，并且又在文档11中出现3次的文档用以下序列表示：15，8，3.</p>
<p>那这三个数字是怎么计算出来的呢？</p>
<p>首先，根据定义TermFreq –&gt; DocDelta[, Freq?]，一个TermFreq结构是由一个DocDelta后面或许跟着Freq组成，也即上面我们说的A+B？结构。</p>
<p>DocDelta自然是想存储包含此Term的文档的ID号了，Freq是在此文档中出现的次数。</p>
<p>所以根据例子，应该存储的完整信息为[DocID = 7, Freq = 1] <a href="见全文检索的基本原理章节">DocID = 11,  Freq = 3</a>。</p>
<p>然而为了节省空间，Lucene对编号此类的数据都是用差值来表示的，也即上面说的规则2，Delta规则，于是文档ID就不能按完整信息存了，就应该存放如下：</p>
<p>[DocIDDelta = 7, Freq = 1][DocIDDelta = 4 (11-7), Freq = 3]</p>
<p>然而Lucene对于A+B?这种或然跟随的结果，有其特殊的存储方式，见规则3，即A+B?规则，如果DocDelta后面跟随的Freq为1，则用DocDelta最后一位置1表示。</p>
<p>如果DocDelta后面跟随的Freq大于1，则DocDelta得最后一位置0，然后后面跟随真正的值，从而对于第一个Term，由于Freq为1，于是放在DocDelta的最后一位表示，DocIDDelta = 7的二进制是000 0111，必须要左移一位，且最后一位置一，000 1111 = 15，对于第二个Term，由于Freq大于一，于是放在DocDelta的最后一位置零，DocIDDelta = 4的二进制是0000 0100，必须要左移一位，且最后一位置零，0000 1000 = 8，然后后面跟随真正的Freq = 3。</p>
<p>于是得到序列：[DocDleta = 15][DocDelta = 8, Freq = 3]，也即序列，15，8，3。</p>
<p>如果omitTf=true，也即我们不在索引中存储一个文档中Term出现的次数，则只存DocID就可以了，因而不存在A+B?规则的应用。</p>
<p>[DocID = 7][DocID = 11]，然后应用规则2，Delta规则，于是得到序列[DocDelta = 7][DocDelta = 4 (11 - 7)]，也即序列，7，4.</p>
</blockquote>
<ul>
<li>对于跳跃表的存储有以下几点需要解释一下：<ul>
<li>跳跃表可根据倒排表本身的长度(DocFreq)和跳跃的幅度(SkipInterval)而分不同的层次，层次数为NumSkipLevels = Min(MaxSkipLevels, floor(log(DocFreq/log(SkipInterval)))).</li>
<li>第Level层的节点数为DocFreq/(SkipInterval^(Level + 1))，level从零计数。</li>
<li>除了最低层之外，其他层都有SkipLevelLength来表示此层的二进制长度(而非节点的个数)，方便读取某一层的跳跃表到缓存里面。</li>
<li>高层在前，低层在后，当读完所有的高层后，剩下的就是最低一层，因而最后一层不需要SkipLevelLength。这也是为什么Lucene文档中的格式描述为 NumSkipLevels-1, SkipLevel，也即低NumSKipLevels-1层有SkipLevelLength，最后一层只有SkipLevel，没有SkipLevelLength。</li>
<li>除最低层以外，其他层都有SkipChildLevelPointer来指向下一层相应的节点。</li>
<li>每一个跳跃节点包含以下信息：文档号，payload的长度，文档号对应的倒排表中的节点在frq中的偏移量，文档号对应的倒排表中的节点在prx中的偏移量。</li>
<li>虽然Lucene的文档中有以下的描述，然而实验的结果却不是完全准确的：</li>
</ul>
</li>
</ul>
<blockquote>
<p>Example: SkipInterval = 4, MaxSkipLevels = 2, DocFreq = 35. Then skip level 0 has 8 SkipData entries, containing the 3rd, 7th, 11th, 15th, 19th, 23rd, 27th, and 31st document numbers in TermFreqs. Skip level 1 has 2 SkipData entries, containing the 15th and 31st document numbers in TermFreqs.</p>
<p>按照描述，当SkipInterval为4，且有35篇文档的时候，Skip level = 0应该包括第3，第7，第11，第15，第19，第23，第27，第31篇文档，Skip level = 1应该包括第15，第31篇文档。</p>
<p>然而真正的实现中，跳跃表节点的时候，却向前偏移了，偏移的原因在于下面的代码：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&gt;FormatPostingsDocsWriter.addDoc(int docID, int termDocFreq)</span><br><span class="line">&gt;	final int delta = docID - lastDocID;</span><br><span class="line">&gt;	if ((++df % skipInterval) == 0)</span><br><span class="line">&gt;		skipListWriter.setSkipData(lastDocID,storePayloads, posWriter.lastPayloadLength);</span><br><span class="line">&gt;		skipListWriter.bufferSkip(df);</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></p>
</blockquote>
<blockquote>
<p>从代码中，我们可以看出，当SkipInterval为4的时候，当docID = 0时，++df为1，1%4不为0，不是跳跃节点，当docID = 3时，++df=4，4%4为0，为跳跃节点，然而skipData里面保存的却是lastDocID为2。</p>
<p>所以真正的倒排表和跳跃表中保存一下的信息：<br><img src="http://oawztil0a.bkt.clouddn.com/Blog/Lucene/Lucene%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6%E6%A0%BC%E5%BC%8F24.png" alt></p>
</blockquote>
<h4 id="2-3、词位置-prx-信息"><a href="#2-3、词位置-prx-信息" class="headerlink" title="2.3、词位置(prx)信息"></a>2.3、词位置(prx)信息</h4><p><img src="http://oawztil0a.bkt.clouddn.com/Blog/Lucene/Lucene%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6%E6%A0%BC%E5%BC%8F25.png" alt><br>词位置信息也是倒排表，也是以跳跃表形式存在的。</p>
<ul>
<li>此文件包含TermCount个项，每一个词都有一项，因为每一个词都有自己的词位置倒排表。</li>
<li>对于每一个词的都有一个DocFreq大小的数组，每项代表一篇文档，记录此文档中此词出现的位置。这个文档数组也是和frq文件中的跳跃表有关系的，从上面我们知道，在frq的跳跃表节点中有ProxSkip，当SkipInterval为3的时候，frq的跳跃表节点指向prx文件中的此数组中的第1，第4，第7，第10，第13，第16篇文档。</li>
<li>对于每一篇文档，可能包含一个词多次，因而有一个Freq大小的数组，每一项代表此词在此文档中出现一次，则有一个位置信息。</li>
<li>每一个位置信息包含：PositionDelta(采用差值规则)，还可以保存payload，应用或然跟随规则。</li>
</ul>
<h3 id="3、其他信息"><a href="#3、其他信息" class="headerlink" title="3、其他信息"></a>3、其他信息</h3><h4 id="3-1、标准化因子文件-nrm"><a href="#3-1、标准化因子文件-nrm" class="headerlink" title="3.1、标准化因子文件(nrm)"></a>3.1、标准化因子文件(nrm)</h4><p>为什么会有标准化因子呢？从第一章中的描述，我们知道，在搜索过程中，搜索出的文档要按与查询语句的相关性排序，相关性大的打分(score)高，从而排在前面。相关性打分(score)使用向量空间模型(Vector Space Model)，在计算相关性之前，要计算Term Weight，也即某Term相对于某Document的重要性。在计算Term Weight时，主要有两个影响因素，一个是此Term在此文档中出现的次数，一个是此Term的普通程度。显然此Term在此文档中出现的次数越多，此Term在此文档中越重要。</p>
<p>这种Term Weight的计算方法是最普通的，然而存在以下几个问题：</p>
<ul>
<li>不同的文档重要性不同。有的文档重要些，有的文档相对不重要，比如对于做软件的，在索引书籍的时候，我想让计算机方面的书更容易搜到，而文学方面的书籍搜索时排名靠后。</li>
<li>不同的域重要性不同。有的域重要一些，如关键字，如标题，有的域不重要一些，如附件等。同样一个词(Term)，出现在关键字中应该比出现在附件中打分要高。</li>
<li>根据词(Term)在文档中出现的绝对次数来决定此词对文档的重要性，有不合理的地方。比如长的文档词在文档中出现的次数相对较多，这样短的文档比较吃亏。比如一个词在一本砖头书中出现了10次，在另外一篇不足100字的文章中出现了9次，就说明砖头书应该排在前面码？不应该，显然此词在不足100字的文章中能出现9次，可见其对此文章的重要性。</li>
</ul>
<p>由于以上原因，Lucene在计算Term Weight时，都会乘上一个标准化因子(Normalization Factor)，来减少上面三个问题的影响。</p>
<p>标准化因子(Normalization Factor)是会影响随后打分(score)的计算的，Lucene的打分计算一部分发生在索引过程中，一般是与查询语句无关的参数如标准化因子，大部分发生在搜索过程中，会在搜索过程的代码分析中详述。</p>
<p>标准化因子(Normalization Factor)在索引过程总的计算如下：<br><img src="http://oawztil0a.bkt.clouddn.com/Blog/Lucene/Lucene%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6%E6%A0%BC%E5%BC%8F26.png" alt></p>
<p>它包括三个参数：</p>
<ul>
<li>Document boost：此值越大，说明此文档越重要。</li>
<li>Field boost：此域越大，说明此域越重要。</li>
<li>lengthNorm(field) = (1.0 / Math.sqrt(numTerms))：一个域中包含的Term总数越多，也即文档越长，此值越小，文档越短，此值越大。</li>
</ul>
<p>从上面的公式，我们知道，一个词(Term)出现在不同的文档或不同的域中，标准化因子不同。比如有两个文档，每个文档有两个域，如果不考虑文档长度，就有四种排列组合，在重要文档的重要域中，在重要文档的非重要域中，在非重要文档的重要域中，在非重要文档的非重要域中，四种组合，每种有不同的标准化因子。</p>
<p>于是在Lucene中，标准化因子共保存了(文档数目乘以域数目)个，格式如下：<br><img src="http://oawztil0a.bkt.clouddn.com/Blog/Lucene/Lucene%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6%E6%A0%BC%E5%BC%8F27.png" alt></p>
<ul>
<li>标准化因子文件(Normalization Factor File: nrm)：<ul>
<li>NormsHeader：字符串“NRM”外加Version，依Lucene的版本的不同而不同。</li>
<li>接着是一个数组，大小为NumFields，每个Field一项，每一项为一个Norms。</li>
<li>Norms也是一个数组，大小为SegSize，即此段中文档的数量，每一项为一个Byte，表示一个浮点数，其中0~2为尾数，3~8为指数。</li>
</ul>
</li>
</ul>
<h4 id="3-2、删除文档文件-del"><a href="#3-2、删除文档文件-del" class="headerlink" title="3.2、删除文档文件(del)"></a>3.2、删除文档文件(del)</h4><p><img src="http://oawztil0a.bkt.clouddn.com/Blog/Lucene/Lucene%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6%E6%A0%BC%E5%BC%8F28.png" alt></p>
<ul>
<li>被删除文档文件(Deleted Document File: .del)<ul>
<li>Format：在此文件中，Bits和DGaps只能保存其中之一，-1表示保存DGaps，非负值表示保存Bits。</li>
<li>ByteCount：此段中有多少文档，就有多少个bit被保存，但是以byte形式计数，也即Bits的大小应该是byte的倍数。</li>
<li>BitCount：Bits中有多少位被至1，表示此文档已经被删除。</li>
<li>Bits：一个数组的byte，大小为ByteCount，应用时被认为是byte*8个bit。</li>
<li>DGaps：如果删除的文档数量很小，则Bits大部分位为0，很浪费空间。DGaps采用以下的方式来保存稀疏数组：比如第十，十二，三十二个文档被删除，于是第十，十二，三十二位设为1，DGaps也是以byte为单位的，仅保存不为0的byte，如第1个byte，第4个byte，第1个byte十进制为20，第4个byte十进制为1。于是保存成DGaps，第1个byte，位置1用不定长正整数保存，值为20用二进制保存，第2个byte，位置4用不定长正整数保存，用差值为3，值为1用二进制保存，二进制数据不用差值表示。</li>
</ul>
</li>
</ul>
<h2 id="五、总体结构"><a href="#五、总体结构" class="headerlink" title="五、总体结构"></a>五、总体结构</h2><p><img src="http://oawztil0a.bkt.clouddn.com/Blog/Lucene/Lucene%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6%E6%A0%BC%E5%BC%8F29.png" alt></p>
<ul>
<li>图示为Lucene索引文件的整体结构：<ul>
<li>属于整个索引(Index)的segment.gen，segment_N，其保存的是段(segment)的元数据信息，然后分多个segment保存数据信息，同一个segment有相同的前缀文件名。</li>
<li>对于每一个段，包含域信息，词信息，以及其他信息(标准化因子，删除文档)</li>
<li>域信息也包括域的元数据信息，在fnm中，域的数据信息，在fdx，fdt中。</li>
<li>词信息是反向信息，包括词典(tis, tii)，文档号及词频倒排表(frq)，词位置倒排表(prx)。</li>
</ul>
</li>
</ul>
<p>大家可以通过看源代码，相应的Reader和Writer来了解文件结构，将更为透彻。</p>
<p>文章参考：<br>觉先博客：<br><a href="http://www.cnblogs.com/forfuture1978/category/300665.html" target="_blank" rel="noopener">http://www.cnblogs.com/forfuture1978/category/300665.html</a><br>Apache Lucene - Index FileFormats：<br><a href="http://lucene.apache.org/java/2_9_0/fileformats.html" target="_blank" rel="noopener">http://lucene.apache.org/java/2_9_0/fileformats.html</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2016/04/09/Lucene/Lucene索引文件格式--详细篇/" data-id="cjtf6o97e00fe0fc5od1eh0s0" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Lucene/">Lucene</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2016/04/20/Geek/PhotoShop更改证件照底色/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          PhotoShop更改证件照底色
        
      </div>
    </a>
  
  
    <a href="/2016/04/03/Java/对象,类,包,模块,组件,容器,框架和架构的关系/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">对象,类,包,模块,组件,容器,框架和架构的关系</div>
    </a>
  
</nav>

  
</article>

</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Algorithm/">Algorithm</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/FreeMarker/">FreeMarker</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Geek/">Geek</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Java/">Java</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Job/">Job</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Linux/">Linux</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Lucene/">Lucene</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/MapReduce/">MapReduce</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Nutch/">Nutch</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Project/">Project</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Python/">Python</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Scala/">Scala</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Scheme/">Scheme</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/kafka/">kafka</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/信息检索/">信息检索</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/健身/">健身</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/操作系统/">操作系统</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/数据库/">数据库</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/数据结构/">数据结构</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/王道学习记录/">王道学习记录</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/网络/">网络</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/Algorithm/">Algorithm</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/FreeMarker/">FreeMarker</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Geek/">Geek</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Java/">Java</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Linux/">Linux</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Lucene/">Lucene</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/MapReduce/">MapReduce</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Maven/">Maven</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/NIO/">NIO</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Nutch/">Nutch</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Project/">Project</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Python/">Python</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Scala/">Scala</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Scheme/">Scheme</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/kafka/">kafka</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/信息检索/">信息检索</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/健身/">健身</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/多线程/">多线程</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/排序算法/">排序算法</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/操作系统/">操作系统</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/数据库/">数据库</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/数据结构/">数据结构</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/框架/">框架</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/王道学习记录/">王道学习记录</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/编程题/">编程题</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/网络/">网络</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/面经/">面经</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/面试笔记/">面试笔记</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/Algorithm/" style="font-size: 17px;">Algorithm</a> <a href="/tags/FreeMarker/" style="font-size: 11px;">FreeMarker</a> <a href="/tags/Geek/" style="font-size: 15px;">Geek</a> <a href="/tags/Java/" style="font-size: 20px;">Java</a> <a href="/tags/Linux/" style="font-size: 12px;">Linux</a> <a href="/tags/Lucene/" style="font-size: 11px;">Lucene</a> <a href="/tags/MapReduce/" style="font-size: 14px;">MapReduce</a> <a href="/tags/Maven/" style="font-size: 10px;">Maven</a> <a href="/tags/NIO/" style="font-size: 11px;">NIO</a> <a href="/tags/Nutch/" style="font-size: 19px;">Nutch</a> <a href="/tags/Project/" style="font-size: 11px;">Project</a> <a href="/tags/Python/" style="font-size: 15px;">Python</a> <a href="/tags/Scala/" style="font-size: 11px;">Scala</a> <a href="/tags/Scheme/" style="font-size: 10px;">Scheme</a> <a href="/tags/kafka/" style="font-size: 15px;">kafka</a> <a href="/tags/信息检索/" style="font-size: 14px;">信息检索</a> <a href="/tags/健身/" style="font-size: 10px;">健身</a> <a href="/tags/多线程/" style="font-size: 18px;">多线程</a> <a href="/tags/排序算法/" style="font-size: 19px;">排序算法</a> <a href="/tags/操作系统/" style="font-size: 13px;">操作系统</a> <a href="/tags/数据库/" style="font-size: 18px;">数据库</a> <a href="/tags/数据结构/" style="font-size: 13px;">数据结构</a> <a href="/tags/框架/" style="font-size: 10px;">框架</a> <a href="/tags/王道学习记录/" style="font-size: 13px;">王道学习记录</a> <a href="/tags/编程题/" style="font-size: 16px;">编程题</a> <a href="/tags/网络/" style="font-size: 11px;">网络</a> <a href="/tags/面经/" style="font-size: 14px;">面经</a> <a href="/tags/面试笔记/" style="font-size: 19px;">面试笔记</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/05/">May 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/04/">April 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/03/">March 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/02/">February 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/01/">January 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/12/">December 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/11/">November 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/10/">October 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/09/">September 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/08/">August 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/07/">July 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/06/">June 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/05/">May 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/04/">April 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/03/">March 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/02/">February 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/01/">January 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/12/">December 2015</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/11/">November 2015</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/10/">October 2015</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/09/">September 2015</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/06/">June 2015</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/05/">May 2015</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/04/">April 2015</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2017/05/22/kafka/kafka详解--性能/">kafka详解--性能</a>
          </li>
        
          <li>
            <a href="/2017/05/22/kafka/Kafka设计解析（一）--Kafka背景及架构介绍/">Kafka设计解析（一）--Kafka背景及架构介绍</a>
          </li>
        
          <li>
            <a href="/2017/04/26/FreeMarker/2017-04-28/">Freemarker入门</a>
          </li>
        
          <li>
            <a href="/2017/04/26/FreeMarker/Freemarker/">Freemarker入门</a>
          </li>
        
          <li>
            <a href="/2017/04/20/Scala/2017-04-24/">Scala学习笔记</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2019 John Doe<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>



  </div>
</body>
</html>